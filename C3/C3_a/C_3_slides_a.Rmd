---
title: 'Class 3a: Review of concepts in Probability and Statistics'
author: "Business Forecasting"
output:
  xaringan::moon_reader:
    self_contained: true
    css: xaringan-themer.css
    lib_dir: libs
    google-font: FALSE 
    nature:
      highlightStyle: github
      countIncrementalSlides: true
      
---   
<style type="text/css">
.remark-slide-content {
    font-size: 20px;
}


</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, dpi = 300)
library(shiny)
library(ggplot2)
library(forecast)
library(plotly)
library(dplyr)
library(igraph)
library(reshape)
library(spData)
library(leaflet)
library(readr)
library(gridExtra)
library(hrbrthemes)
library(cowplot)
library(viridis)
library(gapminder)
library(knitr)
library(kableExtra)
library(DT)
library(MASS)
load("sample_listing.Rda")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#43418A",
colors = c(
  red = "#f34213",
  purple = "#3e2f5b",
  orange = "#ff8811",
  green = "#136f63",
  blue = "#1E90FF",
  white = "#FFFFFF"
))
```
---
## Roadmap

### Last set of classes
- Types of data
- How to describe data
  - With visualizations
  - With summary statistics

--

### This set of classes
- How to evaluate estimators
- How to build confidence intervals
- How to test hypotheses

---

### Motivating Example

1. You run a bunch of Airbnbs

--
2. Should you invest more in cleaning?

--
3. Can you get higher price if your cleanliness score exceeds 4.5?

--
4. Get a sample of listings and compare the price of 
 - Those with cleanliness score below 4.5 (dirty) 
 - and above 4.5 (clean)

```{r, echo=FALSE}
# Load the DT package
library(DT)

Sample_list$clean_label <- ifelse(Sample_list$clean == TRUE, "Clean", "Dirty")

# Display the data table
display_list <- Sample_list
display_list$clean <- display_list$clean_label
display_list$clean_label <- NULL
datatable(display_list,
          fillContainer = FALSE,
          options = list(
            pageLength = 5,
            searching = FALSE,
            initComplete = JS(
              "function(settings, json) {",
              "$(this.api().table().container()).css({'font-size': '12px'});",
              "}"
            )
          ),
          rownames = FALSE
)
```

---
 
### Motivating example

In statistical language: 
- .blue[Population]: Entire group we want to learn about, impossible to assess directly

--
  - All listings of Airbnb in Mexico City
  - Ideally we would like to know the entire distribution of prices
  
--

- .blue[Parameters]: Number describing a characteristic of the population

--
  - We want to know mean price of clean $\mu_c$ and dirty $\mu_d$ apartments 
  
--

- .blue[Sample]: Part of the population we have data for

--
  - We have a sample of 200 listings
  
--

- .blue[Goal]: What we want to learn about the population?
  - Is $\mu_c$ > $\mu_d$? If yes, by how much?
  - But we do not know $\mu_c$ and $\mu_d$
  - We will try to guess it using an estimator and a random IID sample

---



### What is a random sample?

- **At random:** A sample is random if each member of the population (each listing) has an equal chance of being selected. This process of selecting is called *drawing* from a population or a sample.

--
- **Random Variable: $P_i$:**
  - Random variable describing the observation $i$. Before drawing the sample, we don't know its value: it could be any price from the distribution.

--
- **Random Sample** is a collection of random variables $\{P_1, P_2,...,P_n\}$

--
- **Observed Value: $p_i$:**
  - Once we observe a specific outcome for the random variable, it becomes a realized value, or $p_i$. It's no longer a random variable but a constant from our sample.


#### Before Drawing the Sample

```{r, echo=FALSE, results='asis'}
knitr::kable(
  data.frame(
 "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", "", ""),
    "P_2" = c("P_2", "", ""),
    "P_3" = c("P_3", "", ""),
    "P_4" = c("P_4", "", ""),
    "P_5" = c("P_5", "", ""),
    "P_6" = c("P_6", "", ""),
    "P_7" = c("P_7", "", ""),
    "P_8" = c("P_8", "", "")
  ),
  col.names = NULL,
  align = 'c'
)
```
---



### What is a random sample? — Drawing Sample 1

Each time we draw a sample, the random variables $P_i$ get realized into concrete values $p_i$.

#### After Drawing the Sample (Sample 1)

```{r, echo=FALSE, results='asis'}
set.seed(1234)

knitr::kable(
  data.frame(
 "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", sample(1000:9999, 1), 120),
    "P_2" = c("P_2", sample(1000:9999, 1), 150),
    "P_3" = c("P_3", sample(1000:9999, 1), 800),
    "P_4" = c("P_4", sample(1000:9999, 1), 200),
    "P_5" = c("P_5", sample(1000:9999, 1), 1400),
    "P_6" = c("P_6", sample(1000:9999, 1), 110),
    "P_7" = c("P_7", sample(1000:9999, 1), 1800),
    "P_8" = c("P_8", sample(1000:9999, 1), 900)
  ),
  col.names = NULL,
  align = 'c'
)
```
---



### What is a random sample? — Drawing Sample 2

A different random draw from the same population gives different realized values.

#### After Drawing the Sample (Sample 2)

```{r, echo=FALSE, results='asis'}
knitr::kable(
  data.frame(
 "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", sample(1000:9999, 1), 260),
    "P_2" = c("P_2", sample(1000:9999, 1), 420),
    "P_3" = c("P_3", sample(1000:9999, 1), 500),
    "P_4" = c("P_4", sample(1000:9999, 1), 2120),
    "P_5" = c("P_5", sample(1000:9999, 1), 800),
    "P_6" = c("P_6", sample(1000:9999, 1), 1450),
    "P_7" = c("P_7", sample(1000:9999, 1), 120),
    "P_8" = c("P_8", sample(1000:9999, 1), 809)
  ),
  col.names = NULL,
  align = 'c'
)
```
---

### What is a random sample? — Drawing Sample 3

Yet another draw — note how the point estimate changes every time.

#### After Drawing the Sample (Sample 3)

```{r, echo=FALSE, results='asis'}
knitr::kable(
  data.frame(
    "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", sample(1000:9999, 1), 150),
    "P_2" = c("P_2", sample(1000:9999, 1), 980),
    "P_3" = c("P_3", sample(1000:9999, 1), 3450),
    "P_4" = c("P_4", sample(1000:9999, 1), 220),
    "P_5" = c("P_5", sample(1000:9999, 1), 120),
    "P_6" = c("P_6", sample(1000:9999, 1), 853),
    "P_7" = c("P_7", sample(1000:9999, 1), 2353),
    "P_8" = c("P_8", sample(1000:9999, 1), 1244)
  ),
  col.names = NULL,
  align = 'c'
)
```
---


### What is a random sample?



- **IID (Independent and Identically Distributed):**

--
  - **Independent:** The selection of one unit ( $P_i$ ) doesn't affect the selection of another ( $P_j$ )
  
--
  - **Identically Distributed:** All units $P_i$ come from the same distribution.


---

### Estimators


- **Intuition**
  - It's our method of guessing the parameter based on the data we have
  - A function of random variables in our sample $\hat\theta=f(P_1, P_2,...,P_n)$
  - Given its random nature, we can analyze its statistical properties
  - Examples we have seen: 
      - $\hat{\mu_c}=\bar{P}=f(P_1, P_2,...,P_n)=\frac{\sum_n P_i}{n}$
      - $s_c=g(P_1, P_2,...,P_n)=\sqrt{\frac{1}{n-1} \sum^n_{i=1}(P_i-\bar{P})^2}$

--
  - It cannot contain any unknown quantities (like $\sigma$ or $\mu_p$)  
  
--

- **Point Estimate:**
  - A single number computed from the realized sample data $\{p_1,p_2,...p_n\}$
      - $\bar{p}=f(p_1, p_2,...,p_n)=\frac{\sum_n p_i}{n}$
      - No longer random

---

### Example: Estimator

- Suppose we want to know average price of the apartment in Mexico City, but we don't have data for the whole population.

- We take a sample of 8 listings and calculate the average price.

#### Before Drawing the Sample

```{r, echo=FALSE, results='asis'}
knitr::kable(
  data.frame(
 "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", "", ""),
    "P_2" = c("P_2", "", ""),
    "P_3" = c("P_3", "", ""),
    "P_4" = c("P_4", "", ""),
    "P_5" = c("P_5", "", ""),
    "P_6" = c("P_6", "", ""),
    "P_7" = c("P_7", "", ""),
    "P_8" = c("P_8", "", "")
  ),
  col.names = NULL,
  align = 'c'
)
```

**Estimator:** $\hat\mu=\frac{P_1+P_2+P_3+P_4+P_5+P_6+P_7+P_8}{8}$



---



### Example: Estimator

- Suppose we want to know average price of the apartment in Mexico City, but we don't have data for the whole population.

- We take a sample of 8 listings and calculate the average price.

#### After Drawing the Sample (Sample 1)

```{r, echo=FALSE, results='asis'}
set.seed(1234)

knitr::kable(
  data.frame(
 "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", sample(1000:9999, 1), 120),
    "P_2" = c("P_2", sample(1000:9999, 1), 150),
    "P_3" = c("P_3", sample(1000:9999, 1), 800),
    "P_4" = c("P_4", sample(1000:9999, 1), 200),
    "P_5" = c("P_5", sample(1000:9999, 1), 1400),
    "P_6" = c("P_6", sample(1000:9999, 1), 110),
    "P_7" = c("P_7", sample(1000:9999, 1), 1800),
    "P_8" = c("P_8", sample(1000:9999, 1), 900)
  ),
  col.names = NULL,
  align = 'c'
)
```


**Estimator:** $\hat\mu=\frac{P_1+P_2+P_3+P_4+P_5+P_6+P_7+P_8}{8}$

**Point estimate:** $\frac{p_1+p_2+p_3+p_4+p_5+p_6+p_7+p_8}{8}=685$

---

### Example: Estimator

- Suppose we want to know average price of the apartment in Mexico City, but we don't have data for the whole population.

- We take a sample of 8 listings and calculate the average price.

#### After Drawing the Sample (Sample 2)

```{r, echo=FALSE, results='asis'}
knitr::kable(
  data.frame(
 "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", sample(1000:9999, 1), 260),
    "P_2" = c("P_2", sample(1000:9999, 1), 420),
    "P_3" = c("P_3", sample(1000:9999, 1), 500),
    "P_4" = c("P_4", sample(1000:9999, 1), 2120),
    "P_5" = c("P_5", sample(1000:9999, 1), 800),
    "P_6" = c("P_6", sample(1000:9999, 1), 1450),
    "P_7" = c("P_7", sample(1000:9999, 1), 120),
    "P_8" = c("P_8", sample(1000:9999, 1), 809)
  ),
  col.names = NULL,
  align = 'c'
)
```


**Estimator:** $\hat\mu=\frac{P_1+P_2+P_3+P_4+P_5+P_6+P_7+P_8}{8}$

**Point estimate:** $\frac{p_1+p_2+p_3+p_4+p_5+p_6+p_7+p_8}{8}=809.875$

---

### Example: Estimator

- Suppose we want to know average price of the apartment in Mexico City, but we don't have data for the whole population.

- We take a sample of 8 listings and calculate the average price.

#### After Drawing the Sample (Sample 3)

```{r, echo=FALSE, results='asis'}
knitr::kable(
  data.frame(
    "Description" = c("Random Variables P_i (Before Drawing)", "Selected Listings IDs", "Realized Values p_i (After Drawing)"),
    "P_1" = c("P_1", sample(1000:9999, 1), 150),
    "P_2" = c("P_2", sample(1000:9999, 1), 980),
    "P_3" = c("P_3", sample(1000:9999, 1), 3450),
    "P_4" = c("P_4", sample(1000:9999, 1), 220),
    "P_5" = c("P_5", sample(1000:9999, 1), 120),
    "P_6" = c("P_6", sample(1000:9999, 1), 853),
    "P_7" = c("P_7", sample(1000:9999, 1), 2353),
    "P_8" = c("P_8", sample(1000:9999, 1), 1244)
  ),
  col.names = NULL,
  align = 'c'
)
```

**Estimator:** $\hat\mu=\frac{P_1+P_2+P_3+P_4+P_5+P_6+P_7+P_8}{8}$

**Point estimate:** $\frac{p_1+p_2+p_3+p_4+p_5+p_6+p_7+p_8}{8}=1171.25$

---

### Estimators 


- The mean price in our sample is $\bar{p}_c=$ `r format(round(mean(Sample_list[Sample_list$clean=="Clean",]$price),3), scientific=FALSE)` MXN
- This is our point estimate

--
- We can't really say how close this one number (point estimate) is to the true mean price in Mexico City without knowing the population
- But we can say how good our method of guessing (estimator) is by looking at its sampling distribution



---
### Estimators 

- **Sampling distribution** is the distribution of the estimator calculated from multiple random samples drawn from the same population.


`r knitr::include_url('https://www.zoology.ubc.ca/~whitlock/Kingfisher/SamplingNormal.htm', height='500px')`

  
---


### Expectation of an estimator

- A good estimator should be unbiased:
$$E[\hat{\theta}]=\theta$$ 
- Where $\theta$ is some parameter and $\hat{\theta}$ is its estimator
- This should be true for any value of $\theta$
- The sampling distribution should be centered at the parameter's value
- Intuitively, on average the estimator should give us the parameter's value
- When I take many, many, many samples of apartments and calculate the mean price in each sample
  - The average of these means should be super close to the true mean price in Mexico City


--
$$Bias(\hat{\theta})=E[\hat{\theta}]-\theta$$
- Bias of an estimator is a difference between its expectation and the parameter
- Let's look at a couple of estimators and check if they are biased or not

---


### Example 1: Estimator = $X_i$ (a single observation)
#### Expectation
- Consider some random variable $X_i$ with unknown mean $E(X_i)=\mu$
- We want to estimate this mean
- The estimator: $\hat{\theta}_1 = X_i$

--
- Expected Value: $E(\hat{\theta}_1) = E(X_i) = \mu$

--
- Bias: $E(\hat{\theta}_1) - \mu=0$ (unbiased)

--
- Is it a good estimator? (We'll come back to this)
---

### Example 2: Estimator = $(3X_1 + X_2)/5$
#### Expectation
- Consider some random variable $X_i$ with unknown mean $E(X_i)=\mu$
- We want to estimate this mean
- The estimator: $\hat{\theta}_2 = \frac{3X_1 + X_2}{5}$

--
- Expected Value: $E(\hat{\theta}_2)= \frac{3}{5}E(X_1) + \frac{1}{5}E(X_2) = \frac{3}{5}\mu + \frac{1}{5}\mu = \frac{4}{5}\mu$

--
- Bias: $E(\hat{\theta}_2) - \mu= \frac{4}{5}\mu - \mu=-\frac{1}{5}\mu$ (biased)

---
### Example 3: Estimator = $\frac{\sum{X_i}}{n}$ (sample mean)
#### Expectation
- Consider some random variable $X_i$ with unknown mean $E(X_i)=\mu$
- We want to estimate this mean
- The estimator: $\hat{\theta}_3 = \bar{X} = \frac{\sum_n{X_i}}{n}$

--
- Expected Value: $E(\hat{\theta}_3)=E\left(\frac{\sum_n{X_i}}{n}\right)=\frac{\sum_n{E(X_i)}}{n}=\frac{\sum_n{\mu}}{n} = \mu$

--
- Bias: $E(\hat{\theta}_3) - \mu=0$ (unbiased)

---

### Variance of the estimator

- A good estimator is unbiased
- But how do we choose among unbiased estimators?

--
  - Suppose we sample IID from $\small X \sim \mathcal{N}(\mu=10, \sigma=10)$
  - Imagine you don't know the mean is 10, and you try to estimate it:
  - Estimator A:  $\hat\mu_A=\small (3X_1 + X_2)/4$ (note: unbiased since weights sum to 1)
  - Estimator B:  $\hat\mu_B=\small (X_1 + X_2+X_3+X_4)/4$
  - An estimator is more $\textbf{efficient}$ if it has a smaller variance

--
```{r, warning=FALSE, fig.height=3, out.width='100%'}
library(ggplot2)

set.seed(123)
n <- 1000
true_mean <- 10
data <- matrix(rnorm(4 * n, mean = true_mean, sd = 10), ncol = 4)

# Calculate estimators
estimator_1 <- (3 * data[, 1] + 1*data[, 2] ) /4
estimator_2 <- (data[, 1] + data[, 2] + data[, 3]+ data[, 4]) / 4

# Create data frame for plotting
plot_data <- data.frame(
  estimator = c(estimator_1, estimator_2),
  method = rep(c("Estimator A: (3X1+X2)/4", "Estimator B: (X1+X2+X3+X4)/4"), each = n)
)

# Create plot using ggplot2
p <- ggplot(plot_data, aes(x = estimator, fill = method)) +
  geom_histogram(binwidth = 1, color = "black", alpha = 0.7) +
  labs(x = "Estimator Value", y = "Frequency", fill = "Estimator") +
  theme_xaringan() +
  scale_fill_manual(values = c("Estimator A: (3X1+X2)/4" = "blue", "Estimator B: (X1+X2+X3+X4)/4" = "green"))+
  facet_wrap(~method)+
  theme(legend.position = "none")

print(p)


```




---
### Variance of the estimator

- Variance of an estimator is defined as:

$$Var(\hat{\theta})=E[(\hat{\theta}-E[\hat\theta])^2]$$
- We want the estimator to have low  variance! 
- Estimator with the lower variance is more efficient
- Efficiency is a property of unbiased estimators
- In the example above

$$Var(\hat\mu_A)=Var\left(\frac{3X_1 + X_2}{4}\right)>Var\left(\frac{X_1+X_2+X_3+X_4}{4}\right)=Var(\hat\mu_B)$$
- Relative efficiency of two (unbiased) estimators is the ratio of their variances
$$Eff_{\hat\mu_A,\hat\mu_B}=\frac{Var\left(\frac{3X_1 + X_2}{4}\right)}{Var\left(\frac{X_1+X_2+X_3+X_4}{4}\right)}=\frac{\frac{10\sigma^2}{16}}{\frac{4\sigma^2}{16}}=\frac{10}{4}=\frac{5}{2}$$

---

--
#### Example 1: Estimator $\hat{\theta}_1= X_i$

--
- $\small  Var(\hat{\theta}_1) = E[(X_i - \mu)^2]=\sigma^2$

--
#### Example 3: Estimator $\hat{\theta}_3= \frac{\sum{X_i}}{n}$

--
- $\small  Var(\hat{\theta}_3) = E\left[\left(\frac{\sum{X_i}}{n} - \mu\right)^2\right]=\frac{\sigma^2}{n}$

--
#### Estimator A: $\hat\mu_A= \frac{3X_1 + X_2}{4}$

--
- $\small  Var(\hat\mu_A) = E\left[\left((3X_1 + X_2)/4 - \mu\right)^2\right]=\frac{9\sigma^2+\sigma^2}{16}=\frac{10\sigma^2}{16}$


---

### Side note: what if variables are not independent?

- In all previous cases of estimators we assumed an independent sample

- Suppose that $X_1$ and $X_2$ are **not independent**
- Example: daily sales of two products in the same store

--
- $E(X_1+X_2) = E(X_1) + E(X_2)$ (expectation of a sum is always the sum of expectations)

--
- $Var(X_1+X_2) = Var(X_1) + Var(X_2) + 2Cov(X_1,X_2)$

--
- $Var(X_1-X_2) = Var(X_1) + Var(X_2) - 2Cov(X_1,X_2)$



---
### Mean Squared Error

***Mean Squared Error*** (MSE) is a summary measure of how good an estimator is:

$$MSE(\hat{\theta})=E[(\hat{\theta}-\theta)^2]$$
- The lower MSE, the better the estimator


--
- It summarizes both the bias and the variance:

\begin{align*}
\small MSE(\hat{\theta})&=E[(\hat{\theta}-\theta)^2] \\
& =E[(\hat{\theta}-E(\hat{\theta})+E(\hat{\theta})-\theta)^2]\\
&= E[(\hat{\theta} - E(\hat{\theta}))^2 + 2(\hat{\theta} - E(\hat{\theta}))(E(\hat{\theta}) - \theta) + (E(\hat{\theta}) - \theta)^2] \\
&= E[(\hat{\theta} - E(\hat{\theta}))^2] + E[2(\hat{\theta} - E(\hat{\theta}))(E(\hat{\theta}) - \theta)] + E[(E(\hat{\theta}) - \theta)^2] \\
&= E[(\hat{\theta} - E(\hat{\theta}))^2] + 2(\underbrace{E[\hat{\theta} - E(\hat{\theta})]}_{=0})(E(\hat{\theta}) - \theta) + E[(E(\hat{\theta}) - \theta)^2] \\
&= \text{var}(\hat{\theta}) + \text{Bias}(\hat{\theta})^2
\end{align*}


--
- If estimator is unbiased, then $$MSE(\hat{\theta})=\text{var}(\hat{\theta})$$
---

### Trading Bias for Variance

- Suppose you want to estimate customer's income to know who to target.
- Red line shows the true value
- Which of the estimators would you prefer?

```{r, warning=FALSE, fig.height=4.1, out.width='100%'}
set.seed(123)
n <- 1000
true_mean <- 10
data <- rnorm(n, mean = true_mean, sd = 2)

# Define the number of bootstrap samples
num_bootstrap <- 1000

# Initialize vectors to store sample estimates
estimates_efficient <- numeric(num_bootstrap)
estimates_inefficient <- numeric(num_bootstrap)

# Perform bootstrap to estimate the estimators
for (i in 1:num_bootstrap) {
  bootstrap_sample <- sample(data, replace = TRUE)
  estimates_efficient[i] <- mean(bootstrap_sample) - 0.2+ rnorm(1, mean = 0, sd = 0.1)  # Biased and efficient
  estimates_inefficient[i] <- mean(bootstrap_sample) + rnorm(1, mean = 0, sd = 1)  # Unbiased but less efficient
}

# Create data frames for plotting histograms
hist_data_efficient <- data.frame(value = estimates_efficient, estimator = rep("Biased, Low Variance", num_bootstrap))
hist_data_inefficient <- data.frame(value = estimates_inefficient, estimator = rep("Unbiased, High Variance", num_bootstrap))

# Set common x-axis limits
x_limits <- c(5, 15)

# Create histogram plots using ggplot2
binwidth <- 0.1

p1 <- ggplot(hist_data_efficient, aes(x = value)) +
  geom_histogram(binwidth = binwidth, color = "black", fill = "blue", alpha = 0.7) +
  geom_vline(xintercept = true_mean, color = "red", linetype = "dashed", linewidth=2) +
  labs(x = "Estimator Value", y = "Frequency") +
  theme_xaringan() +
  xlim(x_limits)

p2 <- ggplot(hist_data_inefficient, aes(x = value)) +
  geom_histogram(binwidth = binwidth, color = "black", fill = "purple", alpha = 0.7) +
  geom_vline(xintercept = true_mean, color = "red", linetype = "dashed", linewidth=2) +
  labs(x = "Estimator Value", y = "Frequency") +
  theme_xaringan() +
  xlim(x_limits)

# Arrange plots in a 2x1 grid

grid.arrange(p1, p2, ncol = 1)


```


---
### Sampling Distribution

- We know how to determine the mean and the variance of the estimator
- Can we say anything about the distribution of the estimator?

--
- In case of sample mean, yes!

--
- That's what **Central Limit Theorem** is about, the most exciting theorem in statistics!

---

### Central Limit Theorem 

- Suppose $X_1,X_2,...,X_n$ are **i.i.d** variables drawn **at random** from a distribution with mean $\mu$ and standard deviation $\sigma$

- Let $S_n=\sum_nX_n$. 
  
--
  - Note that: $E[S_n]=n\mu$ and $st.dev.(S_n)=\sqrt{n}\sigma$

--
- Let $\bar{X}_n=\frac{\sum_nX_n}{n}$ 

--
  - Note that: $E[\bar{X}_n]=\mu$ and $st.dev.(\bar{X}_n)=\frac{\sigma}{\sqrt{n}}$

--
- Let $Z_n=\frac{\bar{X}_n-\mu}{\frac{\sigma}{\sqrt{n}}}$
  
--
  - Note that: $E[Z_n]=0$ and $st.dev.(Z_n)=1$

--
- **Central Limit Theorem** says that **for large n**:

$$S_n \mathrel{\dot\sim} \mathcal{N}(n\mu,\underbrace{\sqrt n \sigma}_{st.dev.}) \qquad\text{and}\qquad \bar{X}_n \mathrel{\dot\sim} \mathcal{N}(\mu,\frac{\sigma}{\sqrt n}) \qquad\text{and}\qquad Z_n \mathrel{\dot\sim} \mathcal{N}(0,1)$$
- In large samples, sample mean is approximately normally distributed with mean $\mu$ and st. dev. $\frac{\sigma}{\sqrt{n}}$ - As n gets larger, the distribution becomes closer to normal

---
- The original distribution of $X_i$ does not matter (but outliers make convergence longer)
- Larger **n**, tighter distribution around the mean 
- Smaller ** $\sigma$ **, tighter distribution around the mean 
`r knitr::include_url('https://seeing-theory.brown.edu/probability-distributions/index.html#section3', height='450px')`
Source: [Seeing Theory](https://seeing-theory.brown.edu/probability-distributions/index.html#section3)


---
### CLT also works for discrete variables!
- Let $X_i\sim \text{Bernoulli}(p=0.5)$. Here is the distribution of $\bar{X}_n$ for different sample sizes:
```{r, warning=FALSE, fig.height=4, out.width='100%'}
# Set the number of repetitions and the sample sizes
reps <- 10000
sample.sizes <- c(5, 20, 75, 100)

# Set seed for reproducibility
set.seed(123)

# Set smaller margin values to decrease space
par(mfrow = c(2, 2))  # Set mar = c(bottom, left, top, right)

# Outer loop (loop over the sample sizes)
for (n in sample.sizes) {
  
  samplemean <- rep(0, reps) # Initialize the vector of sample means
  stdsamplemean <- rep(0, reps) # Initialize the vector of standardized sample means

  # Inner loop (loop over repetitions)
  for (i in 1:reps) {
    x <- rbinom(n, 1, 0.5)
    samplemean[i] <- mean(x)
    stdsamplemean[i] <- sqrt(n) * (mean(x) - 0.5) / 0.5
  }

  # Plot histogram and overlay the N(0,1) density in every iteration
  hist_data=hist(samplemean, 
       col = "steelblue", 
       breaks = 40,
       xlim = c(0, 1), 
       xlab = paste("n =", n), 
       main = "",
       freq=TRUE,plot=FALSE)

  hist_data$counts <- hist_data$counts / sum(hist_data$counts)

# Plot the histogram with relative frequencies
plot(hist_data, 
     col = "steelblue", 
     xlim = c(0, 1), 
     xlab = paste("n =", n), 
     ylab = "Relative Frequency", 
     main = "")
}

# Reset par to its default settings
par(mfrow = c(1, 1))
```
- What is the standard deviation?

--
- $\sigma_{\bar{X}}=\sqrt{var(\bar{X}_n)}=\frac{\sigma_X}{\sqrt n}=\frac{\sqrt{p(1-p)}}{\sqrt n}=\frac{0.5}{\sqrt n}$



---
### Normal Distribution

Consider the event that a customer who opened the DiDi app will call the car.  Suppose X and Y represent the events that a customer calls a car in Cancun (X) and Puerto Vallarta (Y) respectively. 
- X and Y are Bernoulli variables with probabilities 0.4 and 0.6 respectively
- Suppose you have a random (iid) sample  of 100 customers opening the app from Cancun and 80 from Puerto Vallarta. 
- What is the probability that more than 100 people will call the car?

**Reminders**

$$\text{If } X \sim \mathcal{N}(\mu, \sigma) \text{ and } c \text{ is a constant, then } X + c \sim \mathcal{N}(\mu + c, \sigma)$$
$$\text{If } X \sim \mathcal{N}(\mu, \sigma) \text{ and } c \text{ is a constant, then } cX \sim \mathcal{N}(c\mu, |c|\sigma)$$
$$\text{If } X \sim \mathcal{N}(\mu_1, \sigma_1) \text{ and } Y \sim \mathcal{N}(\mu_2, \sigma_2), \text{ then } X + Y \sim \mathcal{N}(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})$$

---

### What if I don't know $\sigma$

- Suppose that sales in stores are normally distributed with mean 200 and with unknown variance

--
- I want to take a sample of 80 stores and I want to know the probability that the average sales in a sample will be greater than 220

--
$$P(\frac{\sum_{i=1}^{80} X_i}{80}>220)$$
Ok, I know that according to central limit theorem $$\frac{\sum_{i=1}^{80} X_i}{80}\sim N(200, \frac{\sigma}{\sqrt{80}})$$

--
- But if I don't know $\sigma$ how can I use it?
- We can use the sample standard deviation instead to estimate $\sigma$

--
- Since it is just an estimate, it adds uncertainty
- But if you have big sample, then you are really good at estimating standard deviation and the error is small
- So the distribution will still converge to normal, but you will need a bit more observations (say 50 rather than 40)


---
## Sampling distribution of standard deviation

- Great, we now know sample means have a normal distribution in large samples

--

- Can we say something about the sampling distribution of the standard deviation?
- That is, if we take multiple samples and calculate the standard deviation of each sample, what will the distribution of these standard deviations look like?

--

- If $X_i$ comes from a normal distribution, then yes! The sample variance is related to the **chi-squared** distribution
- This will be useful for constructing confidence intervals for the variance


---

## From Normal to Chi-Square
- We start with the standard random normal distribution N(0, 1).
- The transformation $\small X = Z^2$ gives rise to the Chi-Square distribution with 1 degree of freedom $\small  \chi^2(1)$.

--
- The expectation of $\small \chi^2(1)$ is $\small E[X]=E[Z^2]=Var(Z)+E[Z]^2=Var(Z)=1$
- The variance of $\small \chi^2(1)$ is $\small var(X)=2$

```{r, warning=FALSE, fig.height=3, out.width='100%'}
library(ggplot2)

# Generate random data from standard normal distribution
set.seed(42)
z <- rnorm(100000, mean = 0, sd = 1)

# Calculate squared values
x <- z^2

# Create histogram
# Create histogram
p1 <- ggplot(data.frame(x), aes(x = x)) +
  geom_density(fill = "blue", color = "black") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  labs(x = "Value of X",
       y = "Density")+
  theme_xaringan()
p1
```



---

## Visualizing the Connection
- The shaded areas represent probability that $X=Z^2>1$ 
- Where $X \sim \chi^2(1)$ and $Z \sim N(0,1)$
- Shaded parts have the same area in both graphs

```{r, warning=FALSE, fig.height=4, out.width='100%'}
# Generate random data from standard normal distribution
set.seed(42)
z <- rnorm(100000, mean = 0, sd = 1)

# Calculate squared values
x <- z^2
par(mfrow = c(1, 2))

# Calculate histogram, but do not draw it
my_hist=hist(z , breaks=200  , plot=F)
my_hist$counts=my_hist$counts/sum(my_hist$counts)
# Color vector
my_color= ifelse(my_hist$breaks < -1, "purple", ifelse (my_hist$breaks >=1, "purple", rgb(0.2,0.2,0.2,0.2) ))
# Final plot
plot(my_hist, col=my_color , border=F , main="Standard Normal", ylab="Relative Frequency" , xlim=c(-4,4))


# Create plot for Chi-Square distribution
my_hist=hist(x , breaks=200  , plot=F)
my_hist$counts=my_hist$counts/sum(my_hist$counts)
# Color vector
my_color= ifelse(my_hist$breaks >1, "purple", rgb(0.2,0.2,0.2,0.2) )
  # Final plot
plot(my_hist, col=my_color , border=F , main="Chi-Squared(1)", ylab="Relative Frequency" , xlim=c(0,6))

par(mfrow = c(1, 1))
```


---

## Chi-Square and the Sum of Random Normals
- More generally, sum of n iid squared standard normal variables is distributed as Chi-Square with n degrees of freedom
- $\small \sum_nZ^2\sim \chi^2(n)$

--
- The expectation of $\small\chi^2(n)$ is $\small E[X(n)]=E[\sum_n Z_i^2]=\sum_n Var(Z_i)=n$
- The variance of $\small\chi^2(n)$ is $\small var(X)=2n$
<center>
<img src=chi_sq.png width="400">
</center>

--
- Why does the shape converge to normal with large n?

--
- Because of CLT - it's sum of random variables


---